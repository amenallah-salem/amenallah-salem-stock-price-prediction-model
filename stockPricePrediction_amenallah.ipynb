{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stockPricePrediction_amenallah.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "up58MAd6QcqR",
        "colab_type": "code",
        "outputId": "a8cb24b3-7a53-4a4d-b649-da5acc6b27e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow pandas numpy matplotlib yahoo_fin sklearn\n",
        "!pip install requests_html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.1.2)\n",
            "Collecting yahoo_fin\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/bd/27f0066d596c87817b7d8f4a3533fdb666b1649007daee1965751adf07e8/yahoo_fin-0.8.4-py3-none-any.whl\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (42.0.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.1)\n",
            "Installing collected packages: yahoo-fin\n",
            "Successfully installed yahoo-fin-0.8.4\n",
            "Collecting requests_html\n",
            "  Downloading https://files.pythonhosted.org/packages/24/bc/a4380f09bab3a776182578ce6b2771e57259d0d4dbce178205779abdc347/requests_html-0.10.0-py3-none-any.whl\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from requests_html) (0.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from requests_html) (2.21.0)\n",
            "Collecting pyquery\n",
            "  Downloading https://files.pythonhosted.org/packages/78/43/95d42e386c61cb639d1a0b94f0c0b9f0b7d6b981ad3c043a836c8b5bc68b/pyquery-1.4.1-py2.py3-none-any.whl\n",
            "Collecting pyppeteer>=0.0.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/16/a5e8d617994cac605f972523bb25f12e3ff9c30baee29b4a9c50467229d9/pyppeteer-0.0.25.tar.gz (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 20.2MB/s \n",
            "\u001b[?25hCollecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Collecting parse\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/ea/9a16ff916752241aa80f1a5ec56dc6c6defc5d0e70af2d16904a9573367f/parse-1.14.0.tar.gz\n",
            "Collecting w3lib\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/45/1ba17c50a0bb16bd950c9c2b92ec60d40c8ebda9f3371ae4230c437120b6/w3lib-1.21.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->requests_html) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (2.8)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.6/dist-packages (from pyquery->requests_html) (4.2.6)\n",
            "Collecting pyee\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/ce/a9a45667cdffbda2af63baa3ad1adca2feb80abf307d0dc91f9dfdfb3ec3/pyee-7.0.1-py2.py3-none-any.whl\n",
            "Collecting websockets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.6MB/s \n",
            "\u001b[?25hCollecting appdirs\n",
            "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pyppeteer>=0.0.14->requests_html) (4.28.1)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from w3lib->requests_html) (1.12.0)\n",
            "Building wheels for collected packages: pyppeteer, fake-useragent, parse\n",
            "  Building wheel for pyppeteer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyppeteer: filename=pyppeteer-0.0.25-cp36-none-any.whl size=78362 sha256=0a8ba9c17d9ae8031796946c2e9fd78937f06303e70ae2154256a20f8d6dd6e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/e0/5d/070e22eceecf7ecd5fa4b86bbc18c1c7d0b90e02e9b57f35eb\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13485 sha256=62cf5b373878b6f98ec700b6cc72500e092eeaa30780e7b395b75c9206ee12f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.14.0-cp36-none-any.whl size=23463 sha256=0bccf6649e0e6df6c7e8703ddcc8ec05e4c5018cb1896c1cfa9b3b43739e28bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/07/e0/b74bfdc1d434e73ef79e69e301e82a7825e0c070f7442beb61\n",
            "Successfully built pyppeteer fake-useragent parse\n",
            "Installing collected packages: cssselect, pyquery, pyee, websockets, appdirs, pyppeteer, fake-useragent, parse, w3lib, requests-html\n",
            "Successfully installed appdirs-1.4.3 cssselect-1.1.0 fake-useragent-0.1.11 parse-1.14.0 pyee-7.0.1 pyppeteer-0.0.25 pyquery-1.4.1 requests-html-0.10.0 w3lib-1.21.0 websockets-8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Cwohpo1Qver",
        "colab_type": "code",
        "outputId": "f25c1dc0-1dd7-48a7-a5ee-4c8d1252d402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from yahoo_fin import stock_info as si\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "import os\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GsrLFJAQwst",
        "colab_type": "code",
        "outputId": "9abbf1a5-8bdb-4737-ce02-25f8cf5760ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "We are using yahoo_fin module, it is essentially a Python scraper that\n",
        "extracts finance data from Yahoo Finance platform, so it isn't a reliable API,\n",
        "feel free to use other data sources such as Alpha Vantage.\n",
        "\n",
        "\n",
        "'''"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nWe are using yahoo_fin module, it is essentially a Python scraper that\\nextracts finance data from Yahoo Finance platform, so it isn't a reliable API,\\nfeel free to use other data sources such as Alpha Vantage.\\n\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99TErfsyQxCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preparing the Dataset\n",
        "#The following function will downloads the dataset from the Internet and preprocess it:\n",
        "\n",
        "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, \n",
        "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
        "    # see if ticker is already a loaded stock from yahoo finance\n",
        "    if isinstance(ticker, str):\n",
        "        # load it from yahoo_fin library\n",
        "        df = si.get_data(ticker)\n",
        "    elif isinstance(ticker, pd.DataFrame):\n",
        "        # already loaded, use it directly\n",
        "        df = ticker\n",
        "    # this will contain all the elements we want to return from this function\n",
        "    result = {}\n",
        "    # we will also return the original dataframe itself\n",
        "    result['df'] = df.copy()\n",
        "    # make sure that the passed feature_columns exist in the dataframe\n",
        "    for col in feature_columns:\n",
        "        assert col in df.columns\n",
        "    if scale:\n",
        "        column_scaler = {}\n",
        "        # scale the data (prices) from 0 to 1\n",
        "        for column in feature_columns:\n",
        "            scaler = preprocessing.MinMaxScaler()\n",
        "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
        "            column_scaler[column] = scaler\n",
        "\n",
        "        # add the MinMaxScaler instances to the result returned\n",
        "        result[\"column_scaler\"] = column_scaler\n",
        "    # add the target column (label) by shifting by `lookup_step`\n",
        "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
        "    # last `lookup_step` columns contains NaN in future column\n",
        "    # get them before droping NaNs\n",
        "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
        "    # drop NaNs\n",
        "    df.dropna(inplace=True)\n",
        "    sequence_data = []\n",
        "    sequences = deque(maxlen=n_steps)\n",
        "    for entry, target in zip(df[feature_columns].values, df['future'].values):\n",
        "        sequences.append(entry)\n",
        "        if len(sequences) == n_steps:\n",
        "            sequence_data.append([np.array(sequences), target])\n",
        "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
        "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 59 (that is 50+10-1) length\n",
        "    # this last_sequence will be used to predict in future dates that are not available in the dataset\n",
        "    last_sequence = list(sequences) + list(last_sequence)\n",
        "    # shift the last sequence by -1\n",
        "    last_sequence = np.array(pd.DataFrame(last_sequence).shift(-1).dropna())\n",
        "    # add to result\n",
        "    result['last_sequence'] = last_sequence\n",
        "    # construct the X's and y's\n",
        "    X, y = [], []\n",
        "    for seq, target in sequence_data:\n",
        "        X.append(seq)\n",
        "        y.append(target)\n",
        "    # convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    # reshape X to fit the neural network\n",
        "    X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))\n",
        "    # split the dataset\n",
        "    result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n",
        "    # return the result\n",
        "    return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGQjsgYEQw2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model Creation\n",
        "def create_model(input_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
        "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\"):\n",
        "    model = Sequential()\n",
        "    for i in range(n_layers):\n",
        "        if i == 0:\n",
        "            # first layer\n",
        "            model.add(cell(units, return_sequences=True, input_shape=(None, input_length)))\n",
        "        elif i == n_layers - 1:\n",
        "            # last layer\n",
        "            model.add(cell(units, return_sequences=False))\n",
        "        else:\n",
        "            # hidden layers\n",
        "            model.add(cell(units, return_sequences=True))\n",
        "        # add dropout after each layer\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(1, activation=\"linear\"))\n",
        "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCvPq_hzhncA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training our Model\n",
        "# Window size or the sequence length\n",
        "N_STEPS = 50\n",
        "# Lookup step, 1 is the next day\n",
        "LOOKUP_STEP = 1\n",
        "# test ratio size, 0.2 is 20%\n",
        "TEST_SIZE = 0.2\n",
        "# features to use\n",
        "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
        "# date now\n",
        "date_now = time.strftime(\"%Y-%m-%d\")\n",
        "### model parameters\n",
        "N_LAYERS = 3\n",
        "# LSTM cell\n",
        "CELL = LSTM\n",
        "# 256 LSTM neurons\n",
        "UNITS = 256\n",
        "# 40% dropout\n",
        "DROPOUT = 0.4\n",
        "### training parameters\n",
        "# mean squared error loss\n",
        "LOSS = \"mse\"\n",
        "OPTIMIZER = \"rmsprop\"\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 300\n",
        "# Apple stock market\n",
        "ticker = \"AAPL\"\n",
        "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
        "# model name to save\n",
        "model_name = f\"{date_now}_{ticker}-{LOSS}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
        "\n",
        "# create these folders if they does not exist\n",
        "if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "if not os.path.isdir(\"logs\"):\n",
        "    os.mkdir(\"logs\")\n",
        "if not os.path.isdir(\"data\"):\n",
        "    os.mkdir(\"data\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IgwEMHBRf3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the CSV file from disk (dataset) if it already exists (without downloading)\n",
        "if os.path.isfile(ticker_data_filename):\n",
        "    ticker = pd.read_csv(ticker_data_filename)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngEaMrCNRgJ-",
        "colab_type": "code",
        "outputId": "c979b52b-ce32-4688-9788-457cd5656642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# load the data\n",
        "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
        "if not os.path.isfile(ticker_data_filename):\n",
        "    # save the CSV file (dataset)\n",
        "    data[\"df\"].to_csv(ticker_data_filename)\n",
        "# construct the model\n",
        "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
        "                    dropout=DROPOUT, optimizer=OPTIMIZER)\n",
        "# some tensorflow callbacks\n",
        "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name), save_best_only=True, verbose=1)\n",
        "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
        "\n",
        "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
        "                    callbacks=[checkpointer, tensorboard],\n",
        "                    verbose=1)\n",
        "model.save(os.path.join(\"results\", model_name) + \".h5\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 7852 samples, validate on 1964 samples\n",
            "Epoch 1/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 0.0035 - mean_absolute_error: 0.0265\n",
            "Epoch 00001: val_loss improved from inf to 0.00041, saving model to results/2020-02-03_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7852/7852 [==============================] - 6s 797us/sample - loss: 0.0035 - mean_absolute_error: 0.0261 - val_loss: 4.1316e-04 - val_mean_absolute_error: 0.0084\n",
            "Epoch 2/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 0.0019 - mean_absolute_error: 0.0202\n",
            "Epoch 00002: val_loss did not improve from 0.00041\n",
            "7852/7852 [==============================] - 3s 382us/sample - loss: 0.0018 - mean_absolute_error: 0.0201 - val_loss: 0.0010 - val_mean_absolute_error: 0.0157\n",
            "Epoch 3/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 0.0015 - mean_absolute_error: 0.0194\n",
            "Epoch 00003: val_loss improved from 0.00041 to 0.00036, saving model to results/2020-02-03_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 0.0015 - mean_absolute_error: 0.0193 - val_loss: 3.5756e-04 - val_mean_absolute_error: 0.0122\n",
            "Epoch 4/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 0.0014 - mean_absolute_error: 0.0189\n",
            "Epoch 00004: val_loss did not improve from 0.00036\n",
            "7852/7852 [==============================] - 3s 374us/sample - loss: 0.0014 - mean_absolute_error: 0.0189 - val_loss: 0.0011 - val_mean_absolute_error: 0.0171\n",
            "Epoch 5/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0176\n",
            "Epoch 00005: val_loss did not improve from 0.00036\n",
            "7852/7852 [==============================] - 3s 363us/sample - loss: 0.0011 - mean_absolute_error: 0.0175 - val_loss: 0.0010 - val_mean_absolute_error: 0.0183\n",
            "Epoch 6/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 9.4651e-04 - mean_absolute_error: 0.0171\n",
            "Epoch 00006: val_loss improved from 0.00036 to 0.00030, saving model to results/2020-02-03_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7852/7852 [==============================] - 3s 379us/sample - loss: 9.4379e-04 - mean_absolute_error: 0.0171 - val_loss: 2.9991e-04 - val_mean_absolute_error: 0.0110\n",
            "Epoch 7/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 9.8239e-04 - mean_absolute_error: 0.0166\n",
            "Epoch 00007: val_loss did not improve from 0.00030\n",
            "7852/7852 [==============================] - 3s 375us/sample - loss: 9.7799e-04 - mean_absolute_error: 0.0167 - val_loss: 8.6672e-04 - val_mean_absolute_error: 0.0264\n",
            "Epoch 8/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 9.6486e-04 - mean_absolute_error: 0.0167\n",
            "Epoch 00008: val_loss did not improve from 0.00030\n",
            "7852/7852 [==============================] - 3s 362us/sample - loss: 9.6550e-04 - mean_absolute_error: 0.0167 - val_loss: 4.0794e-04 - val_mean_absolute_error: 0.0102\n",
            "Epoch 9/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 8.2618e-04 - mean_absolute_error: 0.0158\n",
            "Epoch 00009: val_loss improved from 0.00030 to 0.00025, saving model to results/2020-02-03_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7852/7852 [==============================] - 3s 377us/sample - loss: 8.2677e-04 - mean_absolute_error: 0.0158 - val_loss: 2.5346e-04 - val_mean_absolute_error: 0.0065\n",
            "Epoch 10/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 8.0472e-04 - mean_absolute_error: 0.0154\n",
            "Epoch 00010: val_loss did not improve from 0.00025\n",
            "7852/7852 [==============================] - 3s 378us/sample - loss: 8.0288e-04 - mean_absolute_error: 0.0154 - val_loss: 4.5810e-04 - val_mean_absolute_error: 0.0102\n",
            "Epoch 11/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 7.4732e-04 - mean_absolute_error: 0.0154\n",
            "Epoch 00011: val_loss improved from 0.00025 to 0.00015, saving model to results/2020-02-03_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7852/7852 [==============================] - 3s 391us/sample - loss: 7.4774e-04 - mean_absolute_error: 0.0154 - val_loss: 1.5109e-04 - val_mean_absolute_error: 0.0058\n",
            "Epoch 12/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 7.8298e-04 - mean_absolute_error: 0.0152\n",
            "Epoch 00012: val_loss did not improve from 0.00015\n",
            "7852/7852 [==============================] - 3s 380us/sample - loss: 7.7932e-04 - mean_absolute_error: 0.0151 - val_loss: 6.9919e-04 - val_mean_absolute_error: 0.0128\n",
            "Epoch 13/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 6.9022e-04 - mean_absolute_error: 0.0146\n",
            "Epoch 00013: val_loss did not improve from 0.00015\n",
            "7852/7852 [==============================] - 3s 369us/sample - loss: 6.9023e-04 - mean_absolute_error: 0.0146 - val_loss: 1.7522e-04 - val_mean_absolute_error: 0.0067\n",
            "Epoch 14/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 6.2551e-04 - mean_absolute_error: 0.0142\n",
            "Epoch 00014: val_loss did not improve from 0.00015\n",
            "7852/7852 [==============================] - 3s 383us/sample - loss: 6.2501e-04 - mean_absolute_error: 0.0142 - val_loss: 7.0899e-04 - val_mean_absolute_error: 0.0174\n",
            "Epoch 15/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 5.8417e-04 - mean_absolute_error: 0.0137\n",
            "Epoch 00015: val_loss did not improve from 0.00015\n",
            "7852/7852 [==============================] - 3s 387us/sample - loss: 5.8923e-04 - mean_absolute_error: 0.0138 - val_loss: 4.0539e-04 - val_mean_absolute_error: 0.0100\n",
            "Epoch 16/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 6.0956e-04 - mean_absolute_error: 0.0138\n",
            "Epoch 00016: val_loss improved from 0.00015 to 0.00013, saving model to results/2020-02-03_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7852/7852 [==============================] - 3s 373us/sample - loss: 6.1210e-04 - mean_absolute_error: 0.0138 - val_loss: 1.3342e-04 - val_mean_absolute_error: 0.0075\n",
            "Epoch 17/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 5.5613e-04 - mean_absolute_error: 0.0135\n",
            "Epoch 00017: val_loss did not improve from 0.00013\n",
            "7852/7852 [==============================] - 3s 372us/sample - loss: 5.5878e-04 - mean_absolute_error: 0.0136 - val_loss: 2.1562e-04 - val_mean_absolute_error: 0.0086\n",
            "Epoch 18/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 5.3355e-04 - mean_absolute_error: 0.0129\n",
            "Epoch 00018: val_loss did not improve from 0.00013\n",
            "7852/7852 [==============================] - 3s 382us/sample - loss: 5.3014e-04 - mean_absolute_error: 0.0128 - val_loss: 2.0595e-04 - val_mean_absolute_error: 0.0056\n",
            "Epoch 19/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 4.9899e-04 - mean_absolute_error: 0.0130\n",
            "Epoch 00019: val_loss improved from 0.00013 to 0.00011, saving model to results/2020-02-03_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7852/7852 [==============================] - 3s 437us/sample - loss: 4.9924e-04 - mean_absolute_error: 0.0131 - val_loss: 1.0724e-04 - val_mean_absolute_error: 0.0078\n",
            "Epoch 20/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 4.9293e-04 - mean_absolute_error: 0.0129\n",
            "Epoch 00020: val_loss did not improve from 0.00011\n",
            "7852/7852 [==============================] - 3s 380us/sample - loss: 4.9135e-04 - mean_absolute_error: 0.0128 - val_loss: 1.2781e-04 - val_mean_absolute_error: 0.0054\n",
            "Epoch 21/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 5.2907e-04 - mean_absolute_error: 0.0130\n",
            "Epoch 00021: val_loss did not improve from 0.00011\n",
            "7852/7852 [==============================] - 3s 408us/sample - loss: 5.2687e-04 - mean_absolute_error: 0.0129 - val_loss: 1.1858e-04 - val_mean_absolute_error: 0.0085\n",
            "Epoch 22/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 4.9638e-04 - mean_absolute_error: 0.0127\n",
            "Epoch 00022: val_loss did not improve from 0.00011\n",
            "7852/7852 [==============================] - 3s 376us/sample - loss: 4.9649e-04 - mean_absolute_error: 0.0127 - val_loss: 3.7248e-04 - val_mean_absolute_error: 0.0100\n",
            "Epoch 23/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 4.7851e-04 - mean_absolute_error: 0.0123\n",
            "Epoch 00023: val_loss did not improve from 0.00011\n",
            "7852/7852 [==============================] - 3s 365us/sample - loss: 4.8520e-04 - mean_absolute_error: 0.0124 - val_loss: 1.4133e-04 - val_mean_absolute_error: 0.0074\n",
            "Epoch 24/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 4.2586e-04 - mean_absolute_error: 0.0122\n",
            "Epoch 00024: val_loss improved from 0.00011 to 0.00005, saving model to results/2020-02-03_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7852/7852 [==============================] - 3s 383us/sample - loss: 4.2389e-04 - mean_absolute_error: 0.0121 - val_loss: 5.0664e-05 - val_mean_absolute_error: 0.0043\n",
            "Epoch 25/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 4.6125e-04 - mean_absolute_error: 0.0120\n",
            "Epoch 00025: val_loss did not improve from 0.00005\n",
            "7852/7852 [==============================] - 3s 368us/sample - loss: 4.6121e-04 - mean_absolute_error: 0.0120 - val_loss: 2.4142e-04 - val_mean_absolute_error: 0.0106\n",
            "Epoch 26/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 4.4382e-04 - mean_absolute_error: 0.0118\n",
            "Epoch 00026: val_loss improved from 0.00005 to 0.00005, saving model to results/2020-02-03_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7852/7852 [==============================] - 3s 384us/sample - loss: 4.4496e-04 - mean_absolute_error: 0.0118 - val_loss: 4.7947e-05 - val_mean_absolute_error: 0.0033\n",
            "Epoch 27/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 4.1283e-04 - mean_absolute_error: 0.0117\n",
            "Epoch 00027: val_loss did not improve from 0.00005\n",
            "7852/7852 [==============================] - 3s 369us/sample - loss: 4.1520e-04 - mean_absolute_error: 0.0118 - val_loss: 4.3291e-04 - val_mean_absolute_error: 0.0094\n",
            "Epoch 28/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 4.1892e-04 - mean_absolute_error: 0.0115\n",
            "Epoch 00028: val_loss did not improve from 0.00005\n",
            "7852/7852 [==============================] - 3s 359us/sample - loss: 4.1995e-04 - mean_absolute_error: 0.0116 - val_loss: 6.6305e-05 - val_mean_absolute_error: 0.0065\n",
            "Epoch 29/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 4.0764e-04 - mean_absolute_error: 0.0115\n",
            "Epoch 00029: val_loss did not improve from 0.00005\n",
            "7852/7852 [==============================] - 3s 379us/sample - loss: 4.0759e-04 - mean_absolute_error: 0.0115 - val_loss: 5.6250e-05 - val_mean_absolute_error: 0.0044\n",
            "Epoch 30/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 3.9580e-04 - mean_absolute_error: 0.0113\n",
            "Epoch 00030: val_loss did not improve from 0.00005\n",
            "7852/7852 [==============================] - 3s 376us/sample - loss: 3.9487e-04 - mean_absolute_error: 0.0113 - val_loss: 4.8335e-05 - val_mean_absolute_error: 0.0039\n",
            "Epoch 31/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 4.0964e-04 - mean_absolute_error: 0.0116\n",
            "Epoch 00031: val_loss did not improve from 0.00005\n",
            "7852/7852 [==============================] - 3s 370us/sample - loss: 4.0908e-04 - mean_absolute_error: 0.0116 - val_loss: 7.2776e-05 - val_mean_absolute_error: 0.0037\n",
            "Epoch 32/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 3.8528e-04 - mean_absolute_error: 0.0113\n",
            "Epoch 00032: val_loss did not improve from 0.00005\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 3.8257e-04 - mean_absolute_error: 0.0113 - val_loss: 7.6745e-05 - val_mean_absolute_error: 0.0075\n",
            "Epoch 33/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 3.6712e-04 - mean_absolute_error: 0.0112\n",
            "Epoch 00033: val_loss improved from 0.00005 to 0.00004, saving model to results/2020-02-03_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7852/7852 [==============================] - 3s 368us/sample - loss: 3.6613e-04 - mean_absolute_error: 0.0112 - val_loss: 4.1100e-05 - val_mean_absolute_error: 0.0030\n",
            "Epoch 34/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 4.0313e-04 - mean_absolute_error: 0.0113\n",
            "Epoch 00034: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 370us/sample - loss: 4.0236e-04 - mean_absolute_error: 0.0112 - val_loss: 2.4259e-04 - val_mean_absolute_error: 0.0079\n",
            "Epoch 35/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 4.1044e-04 - mean_absolute_error: 0.0114\n",
            "Epoch 00035: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 393us/sample - loss: 4.1234e-04 - mean_absolute_error: 0.0114 - val_loss: 9.9120e-05 - val_mean_absolute_error: 0.0075\n",
            "Epoch 36/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 3.7384e-04 - mean_absolute_error: 0.0112\n",
            "Epoch 00036: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 366us/sample - loss: 3.7515e-04 - mean_absolute_error: 0.0112 - val_loss: 1.3886e-04 - val_mean_absolute_error: 0.0078\n",
            "Epoch 37/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 3.4943e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 00037: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 369us/sample - loss: 3.4960e-04 - mean_absolute_error: 0.0109 - val_loss: 1.4694e-04 - val_mean_absolute_error: 0.0072\n",
            "Epoch 38/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 3.8485e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 00038: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 371us/sample - loss: 3.8371e-04 - mean_absolute_error: 0.0109 - val_loss: 7.7864e-05 - val_mean_absolute_error: 0.0059\n",
            "Epoch 39/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 3.5096e-04 - mean_absolute_error: 0.0110\n",
            "Epoch 00039: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 389us/sample - loss: 3.5079e-04 - mean_absolute_error: 0.0110 - val_loss: 1.6589e-04 - val_mean_absolute_error: 0.0108\n",
            "Epoch 40/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 3.3412e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 00040: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 406us/sample - loss: 3.3300e-04 - mean_absolute_error: 0.0108 - val_loss: 2.0609e-04 - val_mean_absolute_error: 0.0104\n",
            "Epoch 41/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 3.6785e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 00041: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 405us/sample - loss: 3.6748e-04 - mean_absolute_error: 0.0108 - val_loss: 7.0673e-05 - val_mean_absolute_error: 0.0058\n",
            "Epoch 42/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 3.2737e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 00042: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 3.2633e-04 - mean_absolute_error: 0.0106 - val_loss: 1.1394e-04 - val_mean_absolute_error: 0.0073\n",
            "Epoch 43/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 3.5169e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 00043: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 360us/sample - loss: 3.4791e-04 - mean_absolute_error: 0.0108 - val_loss: 1.8600e-04 - val_mean_absolute_error: 0.0120\n",
            "Epoch 44/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 3.5003e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 00044: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 358us/sample - loss: 3.5173e-04 - mean_absolute_error: 0.0108 - val_loss: 9.4724e-05 - val_mean_absolute_error: 0.0048\n",
            "Epoch 45/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 3.1671e-04 - mean_absolute_error: 0.0104\n",
            "Epoch 00045: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 374us/sample - loss: 3.2245e-04 - mean_absolute_error: 0.0105 - val_loss: 1.9168e-04 - val_mean_absolute_error: 0.0067\n",
            "Epoch 46/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 3.4283e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 00046: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 384us/sample - loss: 3.4393e-04 - mean_absolute_error: 0.0106 - val_loss: 1.5116e-04 - val_mean_absolute_error: 0.0077\n",
            "Epoch 47/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 3.4170e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 00047: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 370us/sample - loss: 3.4743e-04 - mean_absolute_error: 0.0106 - val_loss: 1.6558e-04 - val_mean_absolute_error: 0.0077\n",
            "Epoch 48/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 3.2571e-04 - mean_absolute_error: 0.0104\n",
            "Epoch 00048: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 372us/sample - loss: 3.2162e-04 - mean_absolute_error: 0.0104 - val_loss: 6.3581e-05 - val_mean_absolute_error: 0.0064\n",
            "Epoch 49/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 3.3011e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 00049: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 374us/sample - loss: 3.3264e-04 - mean_absolute_error: 0.0106 - val_loss: 1.2361e-04 - val_mean_absolute_error: 0.0068\n",
            "Epoch 50/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 3.2228e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 00050: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 383us/sample - loss: 3.2180e-04 - mean_absolute_error: 0.0105 - val_loss: 9.5029e-05 - val_mean_absolute_error: 0.0084\n",
            "Epoch 51/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 3.0396e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 00051: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 377us/sample - loss: 3.0257e-04 - mean_absolute_error: 0.0101 - val_loss: 2.3813e-04 - val_mean_absolute_error: 0.0083\n",
            "Epoch 52/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 3.4960e-04 - mean_absolute_error: 0.0107\n",
            "Epoch 00052: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 367us/sample - loss: 3.4858e-04 - mean_absolute_error: 0.0107 - val_loss: 1.5085e-04 - val_mean_absolute_error: 0.0092\n",
            "Epoch 53/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 3.0267e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 00053: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 355us/sample - loss: 3.0485e-04 - mean_absolute_error: 0.0103 - val_loss: 9.2195e-05 - val_mean_absolute_error: 0.0055\n",
            "Epoch 54/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 3.0911e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 00054: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 3.0642e-04 - mean_absolute_error: 0.0102 - val_loss: 7.3085e-05 - val_mean_absolute_error: 0.0068\n",
            "Epoch 55/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.9641e-04 - mean_absolute_error: 0.0101\n",
            "Epoch 00055: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 408us/sample - loss: 2.9337e-04 - mean_absolute_error: 0.0100 - val_loss: 1.2975e-04 - val_mean_absolute_error: 0.0078\n",
            "Epoch 56/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 3.1187e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 00056: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 367us/sample - loss: 3.0942e-04 - mean_absolute_error: 0.0103 - val_loss: 1.5754e-04 - val_mean_absolute_error: 0.0103\n",
            "Epoch 57/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.8895e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 00057: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 365us/sample - loss: 2.8707e-04 - mean_absolute_error: 0.0101 - val_loss: 1.9382e-04 - val_mean_absolute_error: 0.0066\n",
            "Epoch 58/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 3.0401e-04 - mean_absolute_error: 0.0101\n",
            "Epoch 00058: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 368us/sample - loss: 3.0920e-04 - mean_absolute_error: 0.0102 - val_loss: 2.8437e-04 - val_mean_absolute_error: 0.0094\n",
            "Epoch 59/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.8168e-04 - mean_absolute_error: 0.0101\n",
            "Epoch 00059: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 398us/sample - loss: 2.8302e-04 - mean_absolute_error: 0.0101 - val_loss: 5.4579e-05 - val_mean_absolute_error: 0.0050\n",
            "Epoch 60/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.8527e-04 - mean_absolute_error: 0.0098\n",
            "Epoch 00060: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 404us/sample - loss: 2.8447e-04 - mean_absolute_error: 0.0098 - val_loss: 5.5626e-05 - val_mean_absolute_error: 0.0048\n",
            "Epoch 61/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 3.0263e-04 - mean_absolute_error: 0.0100\n",
            "Epoch 00061: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 417us/sample - loss: 3.0260e-04 - mean_absolute_error: 0.0100 - val_loss: 1.0907e-04 - val_mean_absolute_error: 0.0086\n",
            "Epoch 62/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 3.0133e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 00062: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 369us/sample - loss: 2.9952e-04 - mean_absolute_error: 0.0103 - val_loss: 4.4500e-05 - val_mean_absolute_error: 0.0047\n",
            "Epoch 63/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.9911e-04 - mean_absolute_error: 0.0100\n",
            "Epoch 00063: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 366us/sample - loss: 2.9872e-04 - mean_absolute_error: 0.0100 - val_loss: 4.3647e-05 - val_mean_absolute_error: 0.0046\n",
            "Epoch 64/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 3.0047e-04 - mean_absolute_error: 0.0101\n",
            "Epoch 00064: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 360us/sample - loss: 3.1026e-04 - mean_absolute_error: 0.0102 - val_loss: 5.0934e-05 - val_mean_absolute_error: 0.0040\n",
            "Epoch 65/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.8650e-04 - mean_absolute_error: 0.0100\n",
            "Epoch 00065: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 390us/sample - loss: 2.8501e-04 - mean_absolute_error: 0.0100 - val_loss: 6.9887e-05 - val_mean_absolute_error: 0.0047\n",
            "Epoch 66/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.9810e-04 - mean_absolute_error: 0.0099\n",
            "Epoch 00066: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 385us/sample - loss: 2.9575e-04 - mean_absolute_error: 0.0099 - val_loss: 9.9769e-05 - val_mean_absolute_error: 0.0068\n",
            "Epoch 67/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.9521e-04 - mean_absolute_error: 0.0100\n",
            "Epoch 00067: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 365us/sample - loss: 2.9480e-04 - mean_absolute_error: 0.0100 - val_loss: 8.8204e-05 - val_mean_absolute_error: 0.0064\n",
            "Epoch 68/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.6770e-04 - mean_absolute_error: 0.0099\n",
            "Epoch 00068: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 368us/sample - loss: 2.6744e-04 - mean_absolute_error: 0.0099 - val_loss: 5.0621e-05 - val_mean_absolute_error: 0.0058\n",
            "Epoch 69/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.4817e-04 - mean_absolute_error: 0.0095\n",
            "Epoch 00069: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 372us/sample - loss: 2.4686e-04 - mean_absolute_error: 0.0095 - val_loss: 1.3159e-04 - val_mean_absolute_error: 0.0091\n",
            "Epoch 70/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.8267e-04 - mean_absolute_error: 0.0099\n",
            "Epoch 00070: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 378us/sample - loss: 2.8079e-04 - mean_absolute_error: 0.0099 - val_loss: 1.1008e-04 - val_mean_absolute_error: 0.0045\n",
            "Epoch 71/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.7172e-04 - mean_absolute_error: 0.0097\n",
            "Epoch 00071: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 368us/sample - loss: 2.7337e-04 - mean_absolute_error: 0.0097 - val_loss: 4.6294e-05 - val_mean_absolute_error: 0.0033\n",
            "Epoch 72/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.8363e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00072: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 363us/sample - loss: 2.8309e-04 - mean_absolute_error: 0.0096 - val_loss: 1.4525e-04 - val_mean_absolute_error: 0.0096\n",
            "Epoch 73/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.6529e-04 - mean_absolute_error: 0.0098\n",
            "Epoch 00073: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 2.6540e-04 - mean_absolute_error: 0.0098 - val_loss: 1.9709e-04 - val_mean_absolute_error: 0.0086\n",
            "Epoch 74/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.7562e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00074: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 362us/sample - loss: 2.7600e-04 - mean_absolute_error: 0.0096 - val_loss: 1.0754e-04 - val_mean_absolute_error: 0.0068\n",
            "Epoch 75/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.7746e-04 - mean_absolute_error: 0.0099\n",
            "Epoch 00075: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 408us/sample - loss: 2.7668e-04 - mean_absolute_error: 0.0098 - val_loss: 1.8464e-04 - val_mean_absolute_error: 0.0054\n",
            "Epoch 76/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.6578e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00076: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 370us/sample - loss: 2.6389e-04 - mean_absolute_error: 0.0096 - val_loss: 5.6656e-05 - val_mean_absolute_error: 0.0048\n",
            "Epoch 77/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.9030e-04 - mean_absolute_error: 0.0098\n",
            "Epoch 00077: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 390us/sample - loss: 2.9133e-04 - mean_absolute_error: 0.0098 - val_loss: 1.5136e-04 - val_mean_absolute_error: 0.0057\n",
            "Epoch 78/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.7479e-04 - mean_absolute_error: 0.0097\n",
            "Epoch 00078: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 371us/sample - loss: 2.8220e-04 - mean_absolute_error: 0.0098 - val_loss: 1.0412e-04 - val_mean_absolute_error: 0.0044\n",
            "Epoch 79/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.7256e-04 - mean_absolute_error: 0.0097\n",
            "Epoch 00079: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 368us/sample - loss: 2.7194e-04 - mean_absolute_error: 0.0097 - val_loss: 9.9943e-05 - val_mean_absolute_error: 0.0068\n",
            "Epoch 80/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.7994e-04 - mean_absolute_error: 0.0097\n",
            "Epoch 00080: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 432us/sample - loss: 2.8552e-04 - mean_absolute_error: 0.0097 - val_loss: 4.0787e-04 - val_mean_absolute_error: 0.0118\n",
            "Epoch 81/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.6151e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00081: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 399us/sample - loss: 2.6108e-04 - mean_absolute_error: 0.0096 - val_loss: 4.3520e-05 - val_mean_absolute_error: 0.0033\n",
            "Epoch 82/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.7054e-04 - mean_absolute_error: 0.0097\n",
            "Epoch 00082: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 375us/sample - loss: 2.7333e-04 - mean_absolute_error: 0.0097 - val_loss: 7.1413e-05 - val_mean_absolute_error: 0.0068\n",
            "Epoch 83/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.7416e-04 - mean_absolute_error: 0.0097\n",
            "Epoch 00083: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 365us/sample - loss: 2.7666e-04 - mean_absolute_error: 0.0098 - val_loss: 3.1746e-04 - val_mean_absolute_error: 0.0114\n",
            "Epoch 84/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.6616e-04 - mean_absolute_error: 0.0095\n",
            "Epoch 00084: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 358us/sample - loss: 2.6519e-04 - mean_absolute_error: 0.0095 - val_loss: 1.0522e-04 - val_mean_absolute_error: 0.0071\n",
            "Epoch 85/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.4159e-04 - mean_absolute_error: 0.0093\n",
            "Epoch 00085: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 2.4045e-04 - mean_absolute_error: 0.0093 - val_loss: 2.3418e-04 - val_mean_absolute_error: 0.0092\n",
            "Epoch 86/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.3938e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00086: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 375us/sample - loss: 2.3980e-04 - mean_absolute_error: 0.0094 - val_loss: 3.0864e-04 - val_mean_absolute_error: 0.0120\n",
            "Epoch 87/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.6303e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00087: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 379us/sample - loss: 2.6741e-04 - mean_absolute_error: 0.0095 - val_loss: 1.2843e-04 - val_mean_absolute_error: 0.0075\n",
            "Epoch 88/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.6483e-04 - mean_absolute_error: 0.0095\n",
            "Epoch 00088: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 358us/sample - loss: 2.6364e-04 - mean_absolute_error: 0.0095 - val_loss: 2.2882e-04 - val_mean_absolute_error: 0.0077\n",
            "Epoch 89/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.7144e-04 - mean_absolute_error: 0.0095\n",
            "Epoch 00089: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 366us/sample - loss: 2.7154e-04 - mean_absolute_error: 0.0095 - val_loss: 1.0496e-04 - val_mean_absolute_error: 0.0068\n",
            "Epoch 90/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.6093e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00090: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 367us/sample - loss: 2.6292e-04 - mean_absolute_error: 0.0095 - val_loss: 1.3975e-04 - val_mean_absolute_error: 0.0102\n",
            "Epoch 91/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.5716e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00091: val_loss improved from 0.00004 to 0.00004, saving model to results/2020-02-03_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7852/7852 [==============================] - 3s 375us/sample - loss: 2.5607e-04 - mean_absolute_error: 0.0095 - val_loss: 3.9124e-05 - val_mean_absolute_error: 0.0029\n",
            "Epoch 92/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.5854e-04 - mean_absolute_error: 0.0095\n",
            "Epoch 00092: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 371us/sample - loss: 2.6252e-04 - mean_absolute_error: 0.0095 - val_loss: 1.6199e-04 - val_mean_absolute_error: 0.0044\n",
            "Epoch 93/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.3911e-04 - mean_absolute_error: 0.0093\n",
            "Epoch 00093: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 367us/sample - loss: 2.3836e-04 - mean_absolute_error: 0.0093 - val_loss: 6.4191e-05 - val_mean_absolute_error: 0.0043\n",
            "Epoch 94/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.4508e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00094: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 2.4461e-04 - mean_absolute_error: 0.0091 - val_loss: 6.6494e-05 - val_mean_absolute_error: 0.0038\n",
            "Epoch 95/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.5363e-04 - mean_absolute_error: 0.0095\n",
            "Epoch 00095: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 373us/sample - loss: 2.5587e-04 - mean_absolute_error: 0.0095 - val_loss: 8.8737e-05 - val_mean_absolute_error: 0.0048\n",
            "Epoch 96/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.6548e-04 - mean_absolute_error: 0.0095\n",
            "Epoch 00096: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 401us/sample - loss: 2.6460e-04 - mean_absolute_error: 0.0095 - val_loss: 6.2365e-05 - val_mean_absolute_error: 0.0053\n",
            "Epoch 97/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.3209e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00097: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 376us/sample - loss: 2.3174e-04 - mean_absolute_error: 0.0091 - val_loss: 1.1682e-04 - val_mean_absolute_error: 0.0049\n",
            "Epoch 98/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.4807e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00098: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 373us/sample - loss: 2.4715e-04 - mean_absolute_error: 0.0092 - val_loss: 9.7020e-05 - val_mean_absolute_error: 0.0071\n",
            "Epoch 99/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.4455e-04 - mean_absolute_error: 0.0093\n",
            "Epoch 00099: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 2.4619e-04 - mean_absolute_error: 0.0093 - val_loss: 5.0726e-05 - val_mean_absolute_error: 0.0040\n",
            "Epoch 100/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.3982e-04 - mean_absolute_error: 0.0093\n",
            "Epoch 00100: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 434us/sample - loss: 2.3937e-04 - mean_absolute_error: 0.0093 - val_loss: 5.4918e-05 - val_mean_absolute_error: 0.0050\n",
            "Epoch 101/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.2716e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00101: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 380us/sample - loss: 2.2939e-04 - mean_absolute_error: 0.0092 - val_loss: 3.9900e-05 - val_mean_absolute_error: 0.0051\n",
            "Epoch 102/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.4012e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00102: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 393us/sample - loss: 2.3971e-04 - mean_absolute_error: 0.0091 - val_loss: 1.2446e-04 - val_mean_absolute_error: 0.0079\n",
            "Epoch 103/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.5715e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00103: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 367us/sample - loss: 2.5856e-04 - mean_absolute_error: 0.0092 - val_loss: 5.5233e-05 - val_mean_absolute_error: 0.0036\n",
            "Epoch 104/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.3076e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00104: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 367us/sample - loss: 2.2962e-04 - mean_absolute_error: 0.0091 - val_loss: 7.0837e-05 - val_mean_absolute_error: 0.0074\n",
            "Epoch 105/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.3720e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00105: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 371us/sample - loss: 2.4073e-04 - mean_absolute_error: 0.0091 - val_loss: 1.3149e-04 - val_mean_absolute_error: 0.0044\n",
            "Epoch 106/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.6544e-04 - mean_absolute_error: 0.0093\n",
            "Epoch 00106: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 355us/sample - loss: 2.6467e-04 - mean_absolute_error: 0.0093 - val_loss: 2.1387e-04 - val_mean_absolute_error: 0.0091\n",
            "Epoch 107/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.2647e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00107: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 389us/sample - loss: 2.2589e-04 - mean_absolute_error: 0.0091 - val_loss: 3.9203e-04 - val_mean_absolute_error: 0.0120\n",
            "Epoch 108/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.4891e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00108: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 365us/sample - loss: 2.4886e-04 - mean_absolute_error: 0.0092 - val_loss: 8.4560e-05 - val_mean_absolute_error: 0.0071\n",
            "Epoch 109/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.3479e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00109: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 358us/sample - loss: 2.3393e-04 - mean_absolute_error: 0.0091 - val_loss: 8.5329e-05 - val_mean_absolute_error: 0.0063\n",
            "Epoch 110/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.4903e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00110: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 2.4915e-04 - mean_absolute_error: 0.0091 - val_loss: 8.0912e-05 - val_mean_absolute_error: 0.0073\n",
            "Epoch 111/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.6179e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00111: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 374us/sample - loss: 2.6063e-04 - mean_absolute_error: 0.0094 - val_loss: 5.0329e-05 - val_mean_absolute_error: 0.0053\n",
            "Epoch 112/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.3927e-04 - mean_absolute_error: 0.0093\n",
            "Epoch 00112: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 362us/sample - loss: 2.3882e-04 - mean_absolute_error: 0.0093 - val_loss: 1.5774e-04 - val_mean_absolute_error: 0.0066\n",
            "Epoch 113/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.4253e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00113: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 2.4032e-04 - mean_absolute_error: 0.0091 - val_loss: 6.9005e-05 - val_mean_absolute_error: 0.0044\n",
            "Epoch 114/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.3168e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00114: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 366us/sample - loss: 2.3171e-04 - mean_absolute_error: 0.0091 - val_loss: 4.5035e-05 - val_mean_absolute_error: 0.0032\n",
            "Epoch 115/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.3129e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00115: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 378us/sample - loss: 2.2932e-04 - mean_absolute_error: 0.0091 - val_loss: 6.1624e-05 - val_mean_absolute_error: 0.0029\n",
            "Epoch 116/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.2358e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00116: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 428us/sample - loss: 2.2179e-04 - mean_absolute_error: 0.0090 - val_loss: 6.7607e-05 - val_mean_absolute_error: 0.0064\n",
            "Epoch 117/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.2236e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00117: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 377us/sample - loss: 2.2125e-04 - mean_absolute_error: 0.0089 - val_loss: 4.0437e-05 - val_mean_absolute_error: 0.0032\n",
            "Epoch 118/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.5661e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00118: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 381us/sample - loss: 2.5607e-04 - mean_absolute_error: 0.0092 - val_loss: 2.0187e-04 - val_mean_absolute_error: 0.0096\n",
            "Epoch 119/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.4033e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00119: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 367us/sample - loss: 2.4002e-04 - mean_absolute_error: 0.0091 - val_loss: 4.9976e-05 - val_mean_absolute_error: 0.0059\n",
            "Epoch 120/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.3372e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00120: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 395us/sample - loss: 2.3343e-04 - mean_absolute_error: 0.0091 - val_loss: 5.2862e-05 - val_mean_absolute_error: 0.0042\n",
            "Epoch 121/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.3062e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00121: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 397us/sample - loss: 2.2941e-04 - mean_absolute_error: 0.0090 - val_loss: 2.1971e-04 - val_mean_absolute_error: 0.0119\n",
            "Epoch 122/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.2286e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00122: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 405us/sample - loss: 2.2240e-04 - mean_absolute_error: 0.0091 - val_loss: 1.1647e-04 - val_mean_absolute_error: 0.0058\n",
            "Epoch 123/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.2426e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00123: val_loss did not improve from 0.00004\n",
            "7852/7852 [==============================] - 3s 365us/sample - loss: 2.2652e-04 - mean_absolute_error: 0.0089 - val_loss: 1.2606e-04 - val_mean_absolute_error: 0.0061\n",
            "Epoch 124/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.4115e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00124: val_loss improved from 0.00004 to 0.00003, saving model to results/2020-02-03_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 2.3797e-04 - mean_absolute_error: 0.0091 - val_loss: 2.8518e-05 - val_mean_absolute_error: 0.0028\n",
            "Epoch 125/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.4523e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00125: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 356us/sample - loss: 2.4626e-04 - mean_absolute_error: 0.0091 - val_loss: 1.3000e-04 - val_mean_absolute_error: 0.0059\n",
            "Epoch 126/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.4025e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00126: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 370us/sample - loss: 2.3970e-04 - mean_absolute_error: 0.0090 - val_loss: 5.6839e-05 - val_mean_absolute_error: 0.0041\n",
            "Epoch 127/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.4442e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00127: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 367us/sample - loss: 2.4341e-04 - mean_absolute_error: 0.0090 - val_loss: 7.1493e-05 - val_mean_absolute_error: 0.0073\n",
            "Epoch 128/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.3742e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00128: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 2.3824e-04 - mean_absolute_error: 0.0090 - val_loss: 9.6110e-05 - val_mean_absolute_error: 0.0061\n",
            "Epoch 129/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.2594e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00129: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 367us/sample - loss: 2.2829e-04 - mean_absolute_error: 0.0090 - val_loss: 3.4446e-05 - val_mean_absolute_error: 0.0043\n",
            "Epoch 130/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.1581e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00130: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 373us/sample - loss: 2.1460e-04 - mean_absolute_error: 0.0089 - val_loss: 1.3888e-04 - val_mean_absolute_error: 0.0062\n",
            "Epoch 131/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.2631e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00131: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 379us/sample - loss: 2.2452e-04 - mean_absolute_error: 0.0089 - val_loss: 4.0817e-05 - val_mean_absolute_error: 0.0042\n",
            "Epoch 132/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.1991e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00132: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 362us/sample - loss: 2.1924e-04 - mean_absolute_error: 0.0088 - val_loss: 6.5487e-05 - val_mean_absolute_error: 0.0058\n",
            "Epoch 133/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.2289e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00133: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 366us/sample - loss: 2.2265e-04 - mean_absolute_error: 0.0090 - val_loss: 8.1267e-05 - val_mean_absolute_error: 0.0059\n",
            "Epoch 134/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.1172e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00134: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 374us/sample - loss: 2.1001e-04 - mean_absolute_error: 0.0089 - val_loss: 8.2172e-05 - val_mean_absolute_error: 0.0072\n",
            "Epoch 135/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.4288e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00135: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 362us/sample - loss: 2.4292e-04 - mean_absolute_error: 0.0090 - val_loss: 1.5439e-04 - val_mean_absolute_error: 0.0072\n",
            "Epoch 136/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.2417e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00136: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 396us/sample - loss: 2.2378e-04 - mean_absolute_error: 0.0088 - val_loss: 6.4717e-05 - val_mean_absolute_error: 0.0051\n",
            "Epoch 137/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.4079e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00137: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 362us/sample - loss: 2.3931e-04 - mean_absolute_error: 0.0090 - val_loss: 3.0245e-05 - val_mean_absolute_error: 0.0037\n",
            "Epoch 138/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.3633e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00138: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 381us/sample - loss: 2.3438e-04 - mean_absolute_error: 0.0090 - val_loss: 5.3315e-05 - val_mean_absolute_error: 0.0031\n",
            "Epoch 139/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.0280e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00139: val_loss improved from 0.00003 to 0.00003, saving model to results/2020-02-03_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7852/7852 [==============================] - 3s 373us/sample - loss: 2.0231e-04 - mean_absolute_error: 0.0088 - val_loss: 2.5003e-05 - val_mean_absolute_error: 0.0023\n",
            "Epoch 140/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.3349e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00140: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 368us/sample - loss: 2.3306e-04 - mean_absolute_error: 0.0089 - val_loss: 3.9303e-05 - val_mean_absolute_error: 0.0038\n",
            "Epoch 141/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.2523e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00141: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 441us/sample - loss: 2.2440e-04 - mean_absolute_error: 0.0089 - val_loss: 5.5124e-05 - val_mean_absolute_error: 0.0044\n",
            "Epoch 142/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.1295e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00142: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 402us/sample - loss: 2.1251e-04 - mean_absolute_error: 0.0088 - val_loss: 7.4285e-05 - val_mean_absolute_error: 0.0051\n",
            "Epoch 143/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.2641e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00143: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 379us/sample - loss: 2.2757e-04 - mean_absolute_error: 0.0089 - val_loss: 8.5957e-05 - val_mean_absolute_error: 0.0050\n",
            "Epoch 144/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.2071e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00144: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 362us/sample - loss: 2.2399e-04 - mean_absolute_error: 0.0089 - val_loss: 6.0646e-05 - val_mean_absolute_error: 0.0047\n",
            "Epoch 145/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.1501e-04 - mean_absolute_error: 0.0087\n",
            "Epoch 00145: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 360us/sample - loss: 2.1585e-04 - mean_absolute_error: 0.0088 - val_loss: 2.5402e-04 - val_mean_absolute_error: 0.0083\n",
            "Epoch 146/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.2438e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00146: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 368us/sample - loss: 2.2400e-04 - mean_absolute_error: 0.0088 - val_loss: 2.5706e-05 - val_mean_absolute_error: 0.0028\n",
            "Epoch 147/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.2827e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00147: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 381us/sample - loss: 2.2724e-04 - mean_absolute_error: 0.0089 - val_loss: 4.1786e-05 - val_mean_absolute_error: 0.0028\n",
            "Epoch 148/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.1757e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00148: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 377us/sample - loss: 2.1828e-04 - mean_absolute_error: 0.0088 - val_loss: 5.8869e-05 - val_mean_absolute_error: 0.0062\n",
            "Epoch 149/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.2397e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00149: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 363us/sample - loss: 2.2563e-04 - mean_absolute_error: 0.0090 - val_loss: 5.9213e-05 - val_mean_absolute_error: 0.0039\n",
            "Epoch 150/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.1064e-04 - mean_absolute_error: 0.0086\n",
            "Epoch 00150: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 366us/sample - loss: 2.0900e-04 - mean_absolute_error: 0.0086 - val_loss: 8.0323e-05 - val_mean_absolute_error: 0.0060\n",
            "Epoch 151/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.2309e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00151: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 383us/sample - loss: 2.2284e-04 - mean_absolute_error: 0.0088 - val_loss: 2.0957e-04 - val_mean_absolute_error: 0.0096\n",
            "Epoch 152/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.2624e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00152: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 369us/sample - loss: 2.2708e-04 - mean_absolute_error: 0.0088 - val_loss: 5.0370e-05 - val_mean_absolute_error: 0.0049\n",
            "Epoch 153/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.3304e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00153: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 362us/sample - loss: 2.3277e-04 - mean_absolute_error: 0.0090 - val_loss: 4.1119e-05 - val_mean_absolute_error: 0.0042\n",
            "Epoch 154/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.1209e-04 - mean_absolute_error: 0.0087\n",
            "Epoch 00154: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 370us/sample - loss: 2.1415e-04 - mean_absolute_error: 0.0087 - val_loss: 5.1562e-05 - val_mean_absolute_error: 0.0042\n",
            "Epoch 155/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.0566e-04 - mean_absolute_error: 0.0086\n",
            "Epoch 00155: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 370us/sample - loss: 2.0511e-04 - mean_absolute_error: 0.0086 - val_loss: 8.7864e-05 - val_mean_absolute_error: 0.0037\n",
            "Epoch 156/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.2165e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00156: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 372us/sample - loss: 2.2261e-04 - mean_absolute_error: 0.0089 - val_loss: 2.1835e-04 - val_mean_absolute_error: 0.0101\n",
            "Epoch 157/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.1845e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00157: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 390us/sample - loss: 2.1682e-04 - mean_absolute_error: 0.0087 - val_loss: 4.4859e-05 - val_mean_absolute_error: 0.0052\n",
            "Epoch 158/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.1206e-04 - mean_absolute_error: 0.0086\n",
            "Epoch 00158: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 370us/sample - loss: 2.1151e-04 - mean_absolute_error: 0.0086 - val_loss: 1.0927e-04 - val_mean_absolute_error: 0.0067\n",
            "Epoch 159/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.1909e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00159: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 374us/sample - loss: 2.2265e-04 - mean_absolute_error: 0.0088 - val_loss: 7.6442e-05 - val_mean_absolute_error: 0.0046\n",
            "Epoch 160/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.0456e-04 - mean_absolute_error: 0.0086\n",
            "Epoch 00160: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 366us/sample - loss: 2.0498e-04 - mean_absolute_error: 0.0086 - val_loss: 1.2000e-04 - val_mean_absolute_error: 0.0094\n",
            "Epoch 161/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.1473e-04 - mean_absolute_error: 0.0086\n",
            "Epoch 00161: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 430us/sample - loss: 2.1519e-04 - mean_absolute_error: 0.0086 - val_loss: 1.7238e-04 - val_mean_absolute_error: 0.0062\n",
            "Epoch 162/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.1150e-04 - mean_absolute_error: 0.0087\n",
            "Epoch 00162: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 378us/sample - loss: 2.1097e-04 - mean_absolute_error: 0.0087 - val_loss: 9.6788e-05 - val_mean_absolute_error: 0.0073\n",
            "Epoch 163/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.4523e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00163: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 398us/sample - loss: 2.4340e-04 - mean_absolute_error: 0.0088 - val_loss: 7.0466e-05 - val_mean_absolute_error: 0.0045\n",
            "Epoch 164/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.1784e-04 - mean_absolute_error: 0.0087\n",
            "Epoch 00164: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 374us/sample - loss: 2.1861e-04 - mean_absolute_error: 0.0087 - val_loss: 8.6871e-05 - val_mean_absolute_error: 0.0041\n",
            "Epoch 165/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.0554e-04 - mean_absolute_error: 0.0085\n",
            "Epoch 00165: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 368us/sample - loss: 2.0941e-04 - mean_absolute_error: 0.0085 - val_loss: 2.5622e-04 - val_mean_absolute_error: 0.0083\n",
            "Epoch 166/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.1742e-04 - mean_absolute_error: 0.0087\n",
            "Epoch 00166: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 373us/sample - loss: 2.1729e-04 - mean_absolute_error: 0.0087 - val_loss: 2.0460e-04 - val_mean_absolute_error: 0.0081\n",
            "Epoch 167/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.1243e-04 - mean_absolute_error: 0.0086\n",
            "Epoch 00167: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 370us/sample - loss: 2.1110e-04 - mean_absolute_error: 0.0086 - val_loss: 5.7378e-05 - val_mean_absolute_error: 0.0044\n",
            "Epoch 168/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.0356e-04 - mean_absolute_error: 0.0087\n",
            "Epoch 00168: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 376us/sample - loss: 2.0307e-04 - mean_absolute_error: 0.0087 - val_loss: 7.4348e-05 - val_mean_absolute_error: 0.0060\n",
            "Epoch 169/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.0844e-04 - mean_absolute_error: 0.0087\n",
            "Epoch 00169: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 372us/sample - loss: 2.0935e-04 - mean_absolute_error: 0.0087 - val_loss: 2.1379e-04 - val_mean_absolute_error: 0.0096\n",
            "Epoch 170/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.1593e-04 - mean_absolute_error: 0.0086\n",
            "Epoch 00170: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 371us/sample - loss: 2.1696e-04 - mean_absolute_error: 0.0086 - val_loss: 1.4179e-04 - val_mean_absolute_error: 0.0080\n",
            "Epoch 171/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.0520e-04 - mean_absolute_error: 0.0087\n",
            "Epoch 00171: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 378us/sample - loss: 2.0483e-04 - mean_absolute_error: 0.0086 - val_loss: 4.4331e-05 - val_mean_absolute_error: 0.0051\n",
            "Epoch 172/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.0563e-04 - mean_absolute_error: 0.0085\n",
            "Epoch 00172: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 375us/sample - loss: 2.0604e-04 - mean_absolute_error: 0.0085 - val_loss: 1.4735e-04 - val_mean_absolute_error: 0.0079\n",
            "Epoch 173/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.2732e-04 - mean_absolute_error: 0.0087\n",
            "Epoch 00173: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 367us/sample - loss: 2.2666e-04 - mean_absolute_error: 0.0087 - val_loss: 7.1422e-05 - val_mean_absolute_error: 0.0044\n",
            "Epoch 174/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.9987e-04 - mean_absolute_error: 0.0085\n",
            "Epoch 00174: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 363us/sample - loss: 2.0379e-04 - mean_absolute_error: 0.0086 - val_loss: 1.5724e-04 - val_mean_absolute_error: 0.0080\n",
            "Epoch 175/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.0793e-04 - mean_absolute_error: 0.0086\n",
            "Epoch 00175: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 369us/sample - loss: 2.0564e-04 - mean_absolute_error: 0.0085 - val_loss: 6.0087e-05 - val_mean_absolute_error: 0.0056\n",
            "Epoch 176/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.2023e-04 - mean_absolute_error: 0.0087\n",
            "Epoch 00176: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 366us/sample - loss: 2.1985e-04 - mean_absolute_error: 0.0087 - val_loss: 4.6821e-05 - val_mean_absolute_error: 0.0053\n",
            "Epoch 177/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.0867e-04 - mean_absolute_error: 0.0086\n",
            "Epoch 00177: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 408us/sample - loss: 2.0994e-04 - mean_absolute_error: 0.0086 - val_loss: 6.5106e-05 - val_mean_absolute_error: 0.0039\n",
            "Epoch 178/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.0415e-04 - mean_absolute_error: 0.0085\n",
            "Epoch 00178: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 379us/sample - loss: 2.0346e-04 - mean_absolute_error: 0.0085 - val_loss: 1.0058e-04 - val_mean_absolute_error: 0.0086\n",
            "Epoch 179/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.0336e-04 - mean_absolute_error: 0.0085\n",
            "Epoch 00179: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 377us/sample - loss: 2.0269e-04 - mean_absolute_error: 0.0085 - val_loss: 5.9482e-05 - val_mean_absolute_error: 0.0065\n",
            "Epoch 180/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.9511e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00180: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 384us/sample - loss: 1.9500e-04 - mean_absolute_error: 0.0084 - val_loss: 5.9651e-05 - val_mean_absolute_error: 0.0067\n",
            "Epoch 181/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.0035e-04 - mean_absolute_error: 0.0085\n",
            "Epoch 00181: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 419us/sample - loss: 2.0051e-04 - mean_absolute_error: 0.0085 - val_loss: 5.5408e-05 - val_mean_absolute_error: 0.0038\n",
            "Epoch 182/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.9723e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00182: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 406us/sample - loss: 1.9678e-04 - mean_absolute_error: 0.0084 - val_loss: 3.4875e-05 - val_mean_absolute_error: 0.0030\n",
            "Epoch 183/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.9899e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00183: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 404us/sample - loss: 1.9991e-04 - mean_absolute_error: 0.0084 - val_loss: 7.7378e-05 - val_mean_absolute_error: 0.0071\n",
            "Epoch 184/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.0340e-04 - mean_absolute_error: 0.0085\n",
            "Epoch 00184: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 372us/sample - loss: 2.0347e-04 - mean_absolute_error: 0.0085 - val_loss: 4.4629e-05 - val_mean_absolute_error: 0.0032\n",
            "Epoch 185/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.9464e-04 - mean_absolute_error: 0.0083\n",
            "Epoch 00185: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 366us/sample - loss: 1.9343e-04 - mean_absolute_error: 0.0083 - val_loss: 4.8761e-05 - val_mean_absolute_error: 0.0037\n",
            "Epoch 186/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.9717e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00186: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 384us/sample - loss: 1.9523e-04 - mean_absolute_error: 0.0084 - val_loss: 2.7856e-05 - val_mean_absolute_error: 0.0024\n",
            "Epoch 187/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.0579e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00187: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 370us/sample - loss: 2.0894e-04 - mean_absolute_error: 0.0084 - val_loss: 1.3307e-04 - val_mean_absolute_error: 0.0083\n",
            "Epoch 188/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.9034e-04 - mean_absolute_error: 0.0083\n",
            "Epoch 00188: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 389us/sample - loss: 1.9185e-04 - mean_absolute_error: 0.0084 - val_loss: 6.6842e-05 - val_mean_absolute_error: 0.0069\n",
            "Epoch 189/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.0120e-04 - mean_absolute_error: 0.0085\n",
            "Epoch 00189: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 378us/sample - loss: 1.9914e-04 - mean_absolute_error: 0.0084 - val_loss: 1.4680e-04 - val_mean_absolute_error: 0.0102\n",
            "Epoch 190/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.0471e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00190: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 369us/sample - loss: 2.0429e-04 - mean_absolute_error: 0.0084 - val_loss: 4.8151e-05 - val_mean_absolute_error: 0.0044\n",
            "Epoch 191/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.0291e-04 - mean_absolute_error: 0.0083\n",
            "Epoch 00191: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 365us/sample - loss: 2.0115e-04 - mean_absolute_error: 0.0083 - val_loss: 2.6041e-05 - val_mean_absolute_error: 0.0032\n",
            "Epoch 192/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.0631e-04 - mean_absolute_error: 0.0085\n",
            "Epoch 00192: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 377us/sample - loss: 2.0609e-04 - mean_absolute_error: 0.0085 - val_loss: 8.5903e-05 - val_mean_absolute_error: 0.0080\n",
            "Epoch 193/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.9435e-04 - mean_absolute_error: 0.0085\n",
            "Epoch 00193: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 370us/sample - loss: 1.9485e-04 - mean_absolute_error: 0.0085 - val_loss: 6.5356e-05 - val_mean_absolute_error: 0.0042\n",
            "Epoch 194/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.0505e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00194: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 374us/sample - loss: 2.0475e-04 - mean_absolute_error: 0.0084 - val_loss: 5.5698e-05 - val_mean_absolute_error: 0.0064\n",
            "Epoch 195/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.1477e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00195: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 371us/sample - loss: 2.1424e-04 - mean_absolute_error: 0.0084 - val_loss: 7.6263e-05 - val_mean_absolute_error: 0.0043\n",
            "Epoch 196/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.0190e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00196: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 362us/sample - loss: 2.0120e-04 - mean_absolute_error: 0.0084 - val_loss: 6.0817e-05 - val_mean_absolute_error: 0.0055\n",
            "Epoch 197/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.9570e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00197: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 406us/sample - loss: 1.9497e-04 - mean_absolute_error: 0.0084 - val_loss: 6.9345e-05 - val_mean_absolute_error: 0.0045\n",
            "Epoch 198/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.1791e-04 - mean_absolute_error: 0.0086\n",
            "Epoch 00198: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 376us/sample - loss: 2.1814e-04 - mean_absolute_error: 0.0086 - val_loss: 1.6924e-04 - val_mean_absolute_error: 0.0065\n",
            "Epoch 199/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.0552e-04 - mean_absolute_error: 0.0085\n",
            "Epoch 00199: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 375us/sample - loss: 2.0644e-04 - mean_absolute_error: 0.0085 - val_loss: 6.0377e-05 - val_mean_absolute_error: 0.0041\n",
            "Epoch 200/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.0032e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00200: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 366us/sample - loss: 1.9931e-04 - mean_absolute_error: 0.0084 - val_loss: 6.7146e-05 - val_mean_absolute_error: 0.0054\n",
            "Epoch 201/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.0100e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00201: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 402us/sample - loss: 1.9942e-04 - mean_absolute_error: 0.0083 - val_loss: 3.6933e-05 - val_mean_absolute_error: 0.0030\n",
            "Epoch 202/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.9320e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00202: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 411us/sample - loss: 1.9257e-04 - mean_absolute_error: 0.0084 - val_loss: 7.5378e-05 - val_mean_absolute_error: 0.0047\n",
            "Epoch 203/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.9702e-04 - mean_absolute_error: 0.0083\n",
            "Epoch 00203: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 403us/sample - loss: 1.9842e-04 - mean_absolute_error: 0.0084 - val_loss: 3.9428e-05 - val_mean_absolute_error: 0.0049\n",
            "Epoch 204/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.1135e-04 - mean_absolute_error: 0.0083\n",
            "Epoch 00204: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 376us/sample - loss: 2.1235e-04 - mean_absolute_error: 0.0083 - val_loss: 5.3199e-05 - val_mean_absolute_error: 0.0059\n",
            "Epoch 205/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.9895e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00205: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 360us/sample - loss: 1.9750e-04 - mean_absolute_error: 0.0084 - val_loss: 6.3818e-05 - val_mean_absolute_error: 0.0060\n",
            "Epoch 206/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.0203e-04 - mean_absolute_error: 0.0083\n",
            "Epoch 00206: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 359us/sample - loss: 2.0254e-04 - mean_absolute_error: 0.0083 - val_loss: 3.5135e-04 - val_mean_absolute_error: 0.0133\n",
            "Epoch 207/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.9373e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00207: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 356us/sample - loss: 1.9374e-04 - mean_absolute_error: 0.0084 - val_loss: 9.0703e-05 - val_mean_absolute_error: 0.0079\n",
            "Epoch 208/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.8504e-04 - mean_absolute_error: 0.0083\n",
            "Epoch 00208: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 383us/sample - loss: 1.8388e-04 - mean_absolute_error: 0.0082 - val_loss: 2.7705e-05 - val_mean_absolute_error: 0.0034\n",
            "Epoch 209/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.9662e-04 - mean_absolute_error: 0.0083\n",
            "Epoch 00209: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 374us/sample - loss: 1.9681e-04 - mean_absolute_error: 0.0083 - val_loss: 4.3035e-05 - val_mean_absolute_error: 0.0045\n",
            "Epoch 210/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.9481e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00210: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 358us/sample - loss: 1.9389e-04 - mean_absolute_error: 0.0084 - val_loss: 9.6762e-05 - val_mean_absolute_error: 0.0056\n",
            "Epoch 211/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.9269e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00211: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 369us/sample - loss: 1.9205e-04 - mean_absolute_error: 0.0081 - val_loss: 8.8684e-05 - val_mean_absolute_error: 0.0047\n",
            "Epoch 212/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.9519e-04 - mean_absolute_error: 0.0082\n",
            "Epoch 00212: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 381us/sample - loss: 1.9525e-04 - mean_absolute_error: 0.0082 - val_loss: 3.5365e-05 - val_mean_absolute_error: 0.0029\n",
            "Epoch 213/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.9950e-04 - mean_absolute_error: 0.0082\n",
            "Epoch 00213: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 367us/sample - loss: 1.9929e-04 - mean_absolute_error: 0.0082 - val_loss: 9.0527e-05 - val_mean_absolute_error: 0.0080\n",
            "Epoch 214/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.0747e-04 - mean_absolute_error: 0.0083\n",
            "Epoch 00214: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 370us/sample - loss: 2.0736e-04 - mean_absolute_error: 0.0083 - val_loss: 7.7569e-05 - val_mean_absolute_error: 0.0075\n",
            "Epoch 215/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.8992e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00215: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 365us/sample - loss: 1.8929e-04 - mean_absolute_error: 0.0081 - val_loss: 7.8102e-05 - val_mean_absolute_error: 0.0076\n",
            "Epoch 216/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.1698e-04 - mean_absolute_error: 0.0085\n",
            "Epoch 00216: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 361us/sample - loss: 2.1542e-04 - mean_absolute_error: 0.0084 - val_loss: 3.4786e-05 - val_mean_absolute_error: 0.0032\n",
            "Epoch 217/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.9857e-04 - mean_absolute_error: 0.0082\n",
            "Epoch 00217: val_loss did not improve from 0.00003\n",
            "7852/7852 [==============================] - 3s 399us/sample - loss: 1.9866e-04 - mean_absolute_error: 0.0082 - val_loss: 7.3152e-05 - val_mean_absolute_error: 0.0040\n",
            "Epoch 218/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.0271e-04 - mean_absolute_error: 0.0083\n",
            "Epoch 00218: val_loss improved from 0.00003 to 0.00002, saving model to results/2020-02-03_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7852/7852 [==============================] - 3s 374us/sample - loss: 2.0299e-04 - mean_absolute_error: 0.0083 - val_loss: 2.3143e-05 - val_mean_absolute_error: 0.0029\n",
            "Epoch 219/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.9560e-04 - mean_absolute_error: 0.0082\n",
            "Epoch 00219: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 362us/sample - loss: 1.9843e-04 - mean_absolute_error: 0.0083 - val_loss: 5.1132e-05 - val_mean_absolute_error: 0.0057\n",
            "Epoch 220/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8599e-04 - mean_absolute_error: 0.0082\n",
            "Epoch 00220: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 375us/sample - loss: 1.8550e-04 - mean_absolute_error: 0.0082 - val_loss: 4.6669e-05 - val_mean_absolute_error: 0.0033\n",
            "Epoch 221/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.9547e-04 - mean_absolute_error: 0.0083\n",
            "Epoch 00221: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 409us/sample - loss: 1.9428e-04 - mean_absolute_error: 0.0083 - val_loss: 1.0600e-04 - val_mean_absolute_error: 0.0056\n",
            "Epoch 222/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 2.0398e-04 - mean_absolute_error: 0.0083\n",
            "Epoch 00222: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 4s 449us/sample - loss: 2.0271e-04 - mean_absolute_error: 0.0083 - val_loss: 7.8827e-05 - val_mean_absolute_error: 0.0057\n",
            "Epoch 223/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.1065e-04 - mean_absolute_error: 0.0083\n",
            "Epoch 00223: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 409us/sample - loss: 2.0852e-04 - mean_absolute_error: 0.0083 - val_loss: 7.1955e-05 - val_mean_absolute_error: 0.0078\n",
            "Epoch 224/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.0479e-04 - mean_absolute_error: 0.0084\n",
            "Epoch 00224: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 363us/sample - loss: 2.0284e-04 - mean_absolute_error: 0.0084 - val_loss: 6.6531e-05 - val_mean_absolute_error: 0.0055\n",
            "Epoch 225/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.8471e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00225: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 366us/sample - loss: 1.8582e-04 - mean_absolute_error: 0.0081 - val_loss: 1.0985e-04 - val_mean_absolute_error: 0.0067\n",
            "Epoch 226/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.8737e-04 - mean_absolute_error: 0.0083\n",
            "Epoch 00226: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 359us/sample - loss: 1.8819e-04 - mean_absolute_error: 0.0083 - val_loss: 3.1071e-05 - val_mean_absolute_error: 0.0034\n",
            "Epoch 227/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.9623e-04 - mean_absolute_error: 0.0082\n",
            "Epoch 00227: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 371us/sample - loss: 1.9629e-04 - mean_absolute_error: 0.0083 - val_loss: 4.1672e-05 - val_mean_absolute_error: 0.0049\n",
            "Epoch 228/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8481e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00228: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 376us/sample - loss: 1.8463e-04 - mean_absolute_error: 0.0081 - val_loss: 3.4704e-05 - val_mean_absolute_error: 0.0040\n",
            "Epoch 229/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.9438e-04 - mean_absolute_error: 0.0082\n",
            "Epoch 00229: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 371us/sample - loss: 1.9536e-04 - mean_absolute_error: 0.0082 - val_loss: 9.1865e-05 - val_mean_absolute_error: 0.0054\n",
            "Epoch 230/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.8890e-04 - mean_absolute_error: 0.0082\n",
            "Epoch 00230: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 363us/sample - loss: 1.8905e-04 - mean_absolute_error: 0.0082 - val_loss: 3.9476e-05 - val_mean_absolute_error: 0.0048\n",
            "Epoch 231/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8071e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00231: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 369us/sample - loss: 1.8013e-04 - mean_absolute_error: 0.0081 - val_loss: 1.3218e-04 - val_mean_absolute_error: 0.0099\n",
            "Epoch 232/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.0569e-04 - mean_absolute_error: 0.0083\n",
            "Epoch 00232: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-02-03_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7852/7852 [==============================] - 3s 376us/sample - loss: 2.0491e-04 - mean_absolute_error: 0.0083 - val_loss: 1.9866e-05 - val_mean_absolute_error: 0.0021\n",
            "Epoch 233/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8304e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00233: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 362us/sample - loss: 1.8284e-04 - mean_absolute_error: 0.0080 - val_loss: 6.6544e-05 - val_mean_absolute_error: 0.0049\n",
            "Epoch 234/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.9877e-04 - mean_absolute_error: 0.0082\n",
            "Epoch 00234: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 366us/sample - loss: 1.9981e-04 - mean_absolute_error: 0.0082 - val_loss: 8.4715e-05 - val_mean_absolute_error: 0.0042\n",
            "Epoch 235/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8048e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00235: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 373us/sample - loss: 1.8004e-04 - mean_absolute_error: 0.0080 - val_loss: 7.0921e-05 - val_mean_absolute_error: 0.0061\n",
            "Epoch 236/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.8113e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00236: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 362us/sample - loss: 1.8110e-04 - mean_absolute_error: 0.0080 - val_loss: 8.5493e-05 - val_mean_absolute_error: 0.0074\n",
            "Epoch 237/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.9228e-04 - mean_absolute_error: 0.0082\n",
            "Epoch 00237: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 393us/sample - loss: 1.9413e-04 - mean_absolute_error: 0.0082 - val_loss: 8.7268e-05 - val_mean_absolute_error: 0.0051\n",
            "Epoch 238/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8805e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00238: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 362us/sample - loss: 1.8738e-04 - mean_absolute_error: 0.0080 - val_loss: 6.6043e-05 - val_mean_absolute_error: 0.0034\n",
            "Epoch 239/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8774e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00239: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 377us/sample - loss: 1.8769e-04 - mean_absolute_error: 0.0081 - val_loss: 7.9233e-05 - val_mean_absolute_error: 0.0070\n",
            "Epoch 240/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.9641e-04 - mean_absolute_error: 0.0082\n",
            "Epoch 00240: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 365us/sample - loss: 1.9679e-04 - mean_absolute_error: 0.0082 - val_loss: 1.5664e-04 - val_mean_absolute_error: 0.0104\n",
            "Epoch 241/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.0184e-04 - mean_absolute_error: 0.0082\n",
            "Epoch 00241: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 368us/sample - loss: 2.0175e-04 - mean_absolute_error: 0.0082 - val_loss: 7.6650e-05 - val_mean_absolute_error: 0.0031\n",
            "Epoch 242/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.7446e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00242: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 435us/sample - loss: 1.7524e-04 - mean_absolute_error: 0.0080 - val_loss: 2.0995e-04 - val_mean_absolute_error: 0.0100\n",
            "Epoch 243/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.9454e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00243: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 395us/sample - loss: 1.9533e-04 - mean_absolute_error: 0.0081 - val_loss: 6.8164e-05 - val_mean_absolute_error: 0.0070\n",
            "Epoch 244/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 2.0128e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00244: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 374us/sample - loss: 2.0099e-04 - mean_absolute_error: 0.0081 - val_loss: 6.6042e-05 - val_mean_absolute_error: 0.0046\n",
            "Epoch 245/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.7930e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00245: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 367us/sample - loss: 1.7864e-04 - mean_absolute_error: 0.0081 - val_loss: 6.6417e-05 - val_mean_absolute_error: 0.0043\n",
            "Epoch 246/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.9680e-04 - mean_absolute_error: 0.0082\n",
            "Epoch 00246: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 357us/sample - loss: 1.9710e-04 - mean_absolute_error: 0.0082 - val_loss: 4.0027e-05 - val_mean_absolute_error: 0.0024\n",
            "Epoch 247/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.8502e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00247: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 365us/sample - loss: 1.8932e-04 - mean_absolute_error: 0.0081 - val_loss: 3.9323e-04 - val_mean_absolute_error: 0.0102\n",
            "Epoch 248/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8317e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00248: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 363us/sample - loss: 1.8273e-04 - mean_absolute_error: 0.0080 - val_loss: 4.6021e-05 - val_mean_absolute_error: 0.0035\n",
            "Epoch 249/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8681e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00249: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 373us/sample - loss: 1.8612e-04 - mean_absolute_error: 0.0081 - val_loss: 6.3298e-05 - val_mean_absolute_error: 0.0056\n",
            "Epoch 250/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8384e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00250: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 1.8391e-04 - mean_absolute_error: 0.0080 - val_loss: 3.0480e-05 - val_mean_absolute_error: 0.0032\n",
            "Epoch 251/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.8094e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00251: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 368us/sample - loss: 1.8204e-04 - mean_absolute_error: 0.0080 - val_loss: 3.1351e-05 - val_mean_absolute_error: 0.0029\n",
            "Epoch 252/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.9668e-04 - mean_absolute_error: 0.0082\n",
            "Epoch 00252: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 367us/sample - loss: 1.9833e-04 - mean_absolute_error: 0.0083 - val_loss: 1.4271e-04 - val_mean_absolute_error: 0.0060\n",
            "Epoch 253/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.7922e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00253: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 369us/sample - loss: 1.7887e-04 - mean_absolute_error: 0.0080 - val_loss: 1.7241e-04 - val_mean_absolute_error: 0.0063\n",
            "Epoch 254/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.9630e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00254: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 360us/sample - loss: 1.9520e-04 - mean_absolute_error: 0.0081 - val_loss: 4.8046e-05 - val_mean_absolute_error: 0.0057\n",
            "Epoch 255/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.8539e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00255: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 357us/sample - loss: 1.8611e-04 - mean_absolute_error: 0.0080 - val_loss: 2.4779e-05 - val_mean_absolute_error: 0.0021\n",
            "Epoch 256/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.8698e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00256: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 352us/sample - loss: 1.8629e-04 - mean_absolute_error: 0.0081 - val_loss: 5.6024e-05 - val_mean_absolute_error: 0.0035\n",
            "Epoch 257/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.7484e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00257: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-02-03_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 1.7341e-04 - mean_absolute_error: 0.0080 - val_loss: 1.9717e-05 - val_mean_absolute_error: 0.0025\n",
            "Epoch 258/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.0840e-04 - mean_absolute_error: 0.0083\n",
            "Epoch 00258: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 401us/sample - loss: 2.0798e-04 - mean_absolute_error: 0.0083 - val_loss: 5.8503e-05 - val_mean_absolute_error: 0.0040\n",
            "Epoch 259/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8007e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00259: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 367us/sample - loss: 1.8039e-04 - mean_absolute_error: 0.0080 - val_loss: 1.4094e-04 - val_mean_absolute_error: 0.0050\n",
            "Epoch 260/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.9930e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00260: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 2.0019e-04 - mean_absolute_error: 0.0081 - val_loss: 5.2483e-05 - val_mean_absolute_error: 0.0028\n",
            "Epoch 261/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8078e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00261: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 1.8274e-04 - mean_absolute_error: 0.0081 - val_loss: 1.3353e-04 - val_mean_absolute_error: 0.0060\n",
            "Epoch 262/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.7271e-04 - mean_absolute_error: 0.0079\n",
            "Epoch 00262: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 396us/sample - loss: 1.7376e-04 - mean_absolute_error: 0.0079 - val_loss: 1.4818e-04 - val_mean_absolute_error: 0.0054\n",
            "Epoch 263/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.9219e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00263: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 409us/sample - loss: 1.9038e-04 - mean_absolute_error: 0.0081 - val_loss: 6.8080e-05 - val_mean_absolute_error: 0.0069\n",
            "Epoch 264/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.9501e-04 - mean_absolute_error: 0.0082\n",
            "Epoch 00264: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 390us/sample - loss: 1.9489e-04 - mean_absolute_error: 0.0082 - val_loss: 1.1164e-04 - val_mean_absolute_error: 0.0079\n",
            "Epoch 265/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.9729e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00265: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 363us/sample - loss: 1.9564e-04 - mean_absolute_error: 0.0081 - val_loss: 6.9037e-05 - val_mean_absolute_error: 0.0042\n",
            "Epoch 266/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8265e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00266: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 354us/sample - loss: 1.8233e-04 - mean_absolute_error: 0.0080 - val_loss: 9.1786e-05 - val_mean_absolute_error: 0.0050\n",
            "Epoch 267/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.9272e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00267: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 366us/sample - loss: 1.9286e-04 - mean_absolute_error: 0.0081 - val_loss: 1.0163e-04 - val_mean_absolute_error: 0.0061\n",
            "Epoch 268/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.9184e-04 - mean_absolute_error: 0.0082\n",
            "Epoch 00268: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 366us/sample - loss: 1.9229e-04 - mean_absolute_error: 0.0082 - val_loss: 2.9933e-05 - val_mean_absolute_error: 0.0022\n",
            "Epoch 269/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.7884e-04 - mean_absolute_error: 0.0079\n",
            "Epoch 00269: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 380us/sample - loss: 1.7877e-04 - mean_absolute_error: 0.0079 - val_loss: 6.2341e-05 - val_mean_absolute_error: 0.0042\n",
            "Epoch 270/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.7733e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00270: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 367us/sample - loss: 1.7544e-04 - mean_absolute_error: 0.0080 - val_loss: 9.8346e-05 - val_mean_absolute_error: 0.0074\n",
            "Epoch 271/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.9155e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00271: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 359us/sample - loss: 1.9384e-04 - mean_absolute_error: 0.0081 - val_loss: 4.1508e-05 - val_mean_absolute_error: 0.0045\n",
            "Epoch 272/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.8291e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00272: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 365us/sample - loss: 1.8474e-04 - mean_absolute_error: 0.0080 - val_loss: 4.6036e-05 - val_mean_absolute_error: 0.0049\n",
            "Epoch 273/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.9328e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00273: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 371us/sample - loss: 1.9238e-04 - mean_absolute_error: 0.0080 - val_loss: 3.4592e-05 - val_mean_absolute_error: 0.0045\n",
            "Epoch 274/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8151e-04 - mean_absolute_error: 0.0079\n",
            "Epoch 00274: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 1.8104e-04 - mean_absolute_error: 0.0079 - val_loss: 6.0086e-05 - val_mean_absolute_error: 0.0067\n",
            "Epoch 275/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8247e-04 - mean_absolute_error: 0.0079\n",
            "Epoch 00275: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 363us/sample - loss: 1.8302e-04 - mean_absolute_error: 0.0079 - val_loss: 8.7064e-05 - val_mean_absolute_error: 0.0066\n",
            "Epoch 276/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.7652e-04 - mean_absolute_error: 0.0078\n",
            "Epoch 00276: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 1.7794e-04 - mean_absolute_error: 0.0078 - val_loss: 2.2130e-04 - val_mean_absolute_error: 0.0068\n",
            "Epoch 277/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.7821e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00277: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 1.7741e-04 - mean_absolute_error: 0.0079 - val_loss: 5.2651e-05 - val_mean_absolute_error: 0.0046\n",
            "Epoch 278/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.7683e-04 - mean_absolute_error: 0.0079\n",
            "Epoch 00278: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 391us/sample - loss: 1.7749e-04 - mean_absolute_error: 0.0079 - val_loss: 8.0381e-05 - val_mean_absolute_error: 0.0046\n",
            "Epoch 279/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8514e-04 - mean_absolute_error: 0.0081\n",
            "Epoch 00279: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 368us/sample - loss: 1.8522e-04 - mean_absolute_error: 0.0081 - val_loss: 3.5523e-05 - val_mean_absolute_error: 0.0029\n",
            "Epoch 280/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.7585e-04 - mean_absolute_error: 0.0079\n",
            "Epoch 00280: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 373us/sample - loss: 1.7775e-04 - mean_absolute_error: 0.0079 - val_loss: 7.4561e-05 - val_mean_absolute_error: 0.0045\n",
            "Epoch 281/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.7349e-04 - mean_absolute_error: 0.0078\n",
            "Epoch 00281: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 363us/sample - loss: 1.7310e-04 - mean_absolute_error: 0.0078 - val_loss: 1.4490e-04 - val_mean_absolute_error: 0.0089\n",
            "Epoch 282/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 2.0246e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00282: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 368us/sample - loss: 2.0224e-04 - mean_absolute_error: 0.0080 - val_loss: 8.3806e-05 - val_mean_absolute_error: 0.0049\n",
            "Epoch 283/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.8170e-04 - mean_absolute_error: 0.0079\n",
            "Epoch 00283: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 4s 457us/sample - loss: 1.8126e-04 - mean_absolute_error: 0.0079 - val_loss: 1.0948e-04 - val_mean_absolute_error: 0.0056\n",
            "Epoch 284/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.7989e-04 - mean_absolute_error: 0.0079\n",
            "Epoch 00284: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 403us/sample - loss: 1.7946e-04 - mean_absolute_error: 0.0079 - val_loss: 2.4770e-05 - val_mean_absolute_error: 0.0035\n",
            "Epoch 285/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.8085e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00285: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 383us/sample - loss: 1.7968e-04 - mean_absolute_error: 0.0080 - val_loss: 3.4433e-05 - val_mean_absolute_error: 0.0029\n",
            "Epoch 286/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.8583e-04 - mean_absolute_error: 0.0080\n",
            "Epoch 00286: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 364us/sample - loss: 1.8428e-04 - mean_absolute_error: 0.0079 - val_loss: 7.4226e-05 - val_mean_absolute_error: 0.0059\n",
            "Epoch 287/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8385e-04 - mean_absolute_error: 0.0079\n",
            "Epoch 00287: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 351us/sample - loss: 1.8369e-04 - mean_absolute_error: 0.0080 - val_loss: 3.6829e-05 - val_mean_absolute_error: 0.0044\n",
            "Epoch 288/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.7705e-04 - mean_absolute_error: 0.0078\n",
            "Epoch 00288: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 368us/sample - loss: 1.7723e-04 - mean_absolute_error: 0.0079 - val_loss: 3.0359e-05 - val_mean_absolute_error: 0.0044\n",
            "Epoch 289/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.7443e-04 - mean_absolute_error: 0.0079\n",
            "Epoch 00289: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 373us/sample - loss: 1.7413e-04 - mean_absolute_error: 0.0079 - val_loss: 4.4859e-05 - val_mean_absolute_error: 0.0046\n",
            "Epoch 290/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.6979e-04 - mean_absolute_error: 0.0077\n",
            "Epoch 00290: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 373us/sample - loss: 1.7089e-04 - mean_absolute_error: 0.0077 - val_loss: 3.2770e-04 - val_mean_absolute_error: 0.0059\n",
            "Epoch 291/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.7064e-04 - mean_absolute_error: 0.0078\n",
            "Epoch 00291: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 360us/sample - loss: 1.7028e-04 - mean_absolute_error: 0.0078 - val_loss: 7.3187e-05 - val_mean_absolute_error: 0.0062\n",
            "Epoch 292/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.8199e-04 - mean_absolute_error: 0.0079\n",
            "Epoch 00292: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 369us/sample - loss: 1.8334e-04 - mean_absolute_error: 0.0079 - val_loss: 6.3820e-05 - val_mean_absolute_error: 0.0048\n",
            "Epoch 293/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.9053e-04 - mean_absolute_error: 0.0079\n",
            "Epoch 00293: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 369us/sample - loss: 1.9104e-04 - mean_absolute_error: 0.0079 - val_loss: 9.6322e-05 - val_mean_absolute_error: 0.0068\n",
            "Epoch 294/300\n",
            "7744/7852 [============================>.] - ETA: 0s - loss: 1.7588e-04 - mean_absolute_error: 0.0078\n",
            "Epoch 00294: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 388us/sample - loss: 1.7627e-04 - mean_absolute_error: 0.0078 - val_loss: 5.1366e-05 - val_mean_absolute_error: 0.0045\n",
            "Epoch 295/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.7323e-04 - mean_absolute_error: 0.0078\n",
            "Epoch 00295: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 368us/sample - loss: 1.7455e-04 - mean_absolute_error: 0.0078 - val_loss: 4.8884e-05 - val_mean_absolute_error: 0.0048\n",
            "Epoch 296/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.7265e-04 - mean_absolute_error: 0.0077\n",
            "Epoch 00296: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 371us/sample - loss: 1.7217e-04 - mean_absolute_error: 0.0077 - val_loss: 3.9819e-05 - val_mean_absolute_error: 0.0028\n",
            "Epoch 297/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.8631e-04 - mean_absolute_error: 0.0079\n",
            "Epoch 00297: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 362us/sample - loss: 1.8625e-04 - mean_absolute_error: 0.0079 - val_loss: 6.8705e-05 - val_mean_absolute_error: 0.0059\n",
            "Epoch 298/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.7730e-04 - mean_absolute_error: 0.0079\n",
            "Epoch 00298: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 360us/sample - loss: 1.8042e-04 - mean_absolute_error: 0.0080 - val_loss: 1.7532e-04 - val_mean_absolute_error: 0.0067\n",
            "Epoch 299/300\n",
            "7808/7852 [============================>.] - ETA: 0s - loss: 1.6716e-04 - mean_absolute_error: 0.0079\n",
            "Epoch 00299: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 399us/sample - loss: 1.6713e-04 - mean_absolute_error: 0.0079 - val_loss: 5.6476e-05 - val_mean_absolute_error: 0.0048\n",
            "Epoch 300/300\n",
            "7680/7852 [============================>.] - ETA: 0s - loss: 1.6681e-04 - mean_absolute_error: 0.0076\n",
            "Epoch 00300: val_loss did not improve from 0.00002\n",
            "7852/7852 [==============================] - 3s 365us/sample - loss: 1.6661e-04 - mean_absolute_error: 0.0076 - val_loss: 8.2622e-05 - val_mean_absolute_error: 0.0056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErwKxcFECNAF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4cbc92af-f898-4dda-8d90-5e216e483f5d"
      },
      "source": [
        "#Testing the Model\n",
        "# evaluate the model\n",
        "mse, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"])\n",
        "# calculate the mean absolute error (inverse scaling)\n",
        "mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform(mae.reshape(1, -1))[0][0]\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1964/1964 [==============================] - 0s 201us/sample - loss: 8.2622e-05 - mean_absolute_error: 0.0056\n",
            "Mean Absolute Error: 1.9642618\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGqBGS64CPgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#let's try to predict the future price of Apple Stock Market:\n",
        "def predict(model, data, classification=False):\n",
        "    # retrieve the last sequence from data\n",
        "    last_sequence = data[\"last_sequence\"][:N_STEPS]\n",
        "    # retrieve the column scalers\n",
        "    column_scaler = data[\"column_scaler\"]\n",
        "    # reshape the last sequence\n",
        "    last_sequence = last_sequence.reshape((last_sequence.shape[1], last_sequence.shape[0]))\n",
        "    # expand dimension\n",
        "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
        "    # get the prediction (scaled from 0 to 1)\n",
        "    prediction = model.predict(last_sequence)\n",
        "    # get the price (by inverting the scaling)\n",
        "    predicted_price = column_scaler[\"adjclose\"].inverse_transform(prediction)[0][0]\n",
        "    return predicted_price\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FfkQMHICQiU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7638fef-27ce-411a-8eeb-9dc845b2ca71"
      },
      "source": [
        "# predict the future price\n",
        "future_price = predict(model, data)\n",
        "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Future price after 1 days is 320.48$\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKNB-a7aCRH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The last price was 298.45$, the model is saying that the next day,\n",
        "#it will be 308.20$. The model just used 50 days of features to be able to get that value, \n",
        "#Now plot the prices and see:\n",
        "def plot_graph(model, data):\n",
        "    y_test = data[\"y_test\"]\n",
        "    X_test = data[\"X_test\"]\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
        "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
        "    plt.plot(y_test[-200:], c='b')\n",
        "    plt.plot(y_pred[-200:], c='r')\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVrJrWdfKMkw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "3b4f8eed-334a-4c89-d57e-be16b3e66d80"
      },
      "source": [
        "plot_graph(model, data)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxUBe2S7KRDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_accuracy(model, data):\n",
        "    y_test = data[\"y_test\"]\n",
        "    X_test = data[\"X_test\"]\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
        "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
        "    y_pred = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_pred[LOOKUP_STEP:]))\n",
        "    y_test = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_test[LOOKUP_STEP:]))\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5MtwBTlKSt8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "438b094d-daf8-47b5-bcc8-e66dc005145c"
      },
      "source": [
        "print(\"Accuracy Score:\", get_accuracy(model, data))\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score: 0.8537952114111055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUxksrEQKXs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}